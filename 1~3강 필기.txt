지수이동평균선, 이동평균선 - w가 뭐냐(window)
벨만방정식 wtf
on-policy: 행동 결정하는 정책과 학습할 정책이 같으면 정책이 업데이트될 경우 경험을 실컷
쌓아도 이 경험을 학습에 사용할 수 없다? wtf

이용 = 탐욕적으로 행동 선택 = 알고있는 한의 최적의 행동 선택
탐험 = 아무 행동이나 선택 -> 지역최적화 오류 해결방안

몬테카를로 예측 - 상태 가치 함수 사용

몬테카를로 제어 - 상태 행동 가치 함수 사용 - 탐험 확률적으로 진행 가능

on-policy - SARSA(s->a->r->s->a)
off-policy - Q-learning - regression
	-> 기대 손익 수치적으로 예측 - 예측값 높은 쪽으로 행동
policy gradient - classification
	-> 현 상태 좋은지 확률적으로 판단
actor-critic: Q-learning과 policy gradient의 하이브리드
	actor - policy gradient역할 - 행동 결정 - 정책신경망 학습
	critic - Q-learning으로 행동 비평 -> 단일 모델 사용시의 고분산 문제 해결
					- 상태-행동 가치 신경망 학습
A2C - actor-critic에서 기대출력을 advantage(상태-행동 가치 - 상태 가치)로 사용
			critic을 advantage로 사용
A3C - A2C에서 asynchronous(비동기) 개념 추가 - 여러 환경에서 에이전트가 동시에
					신경망 학습(독립적으로 업데이트됨)
강화학습 예시) 알파고, 벽돌깨기

에포크 - 학습 횟수
엡실론 - 탐험의 비율 - 점점 적어지게
배치학습 - 지연 보상으로 학습 데이터 생성 후 한꺼번에 신경망 업데이트 -> 가중치 업데이트
지연보상 기준 낮으면 작은 배치 학습 데이터로 학습빈도는 커짐

강화학습 환경: 주식 종목의 일봉 차트
행동: 매도 매수 관망

<RLTrader>
data 폴더: 차트 데이터, 학습 데이터 저장, 학습 완료한 가치 신경망, 정책 신경망 모델 저장
output 폴더: 강화학습 과정 로그 txt
->epoch_summary 폴더: 에포크마다 발생하는 가시화 결과 저장

learners.py - Reinforcement Learner & networks.py - Network
=> 가치 신경망, 정책 신경망 학습에 활용
<environment.py - Environment>
=> 에이전트가 투자할 종목의 차트 데이터 관리 
but 순차적으로 제공 - 과거로 돌아간 에이전트는 미래의 차트 모른다
속성) 
- chart_data: 주식 종목의 차트 데이터
- observation: 현재 관측치
- idx: 차트 데이터의 현재 위치
함수)
- reset(): idx, observation 초기화
- observe(): idx를 다음 위치로 이동하고 observation 업데이트
- get_price(): 현재 observation에서 종가 획득

<agent.py - Agent>
=> 매수, 매도(투자자 역할), 상태(초기 자본금, 현금 잔고, 주식 잔고), 
	포트폴리오 가치(PV) - 주식만 고려함
	PV = 주식잔고 X 현재 주가 + 현금 잔고 : PV 높이면 수익up - 높이는게 목표
속성)
- initial_balance: 초기 투자금
- balance: 현금 잔고
- num_stocks: 보유 주식 수
- portfolio_value: 포트폴리오 가치
- num_buy: 매수 횟수
- num_sell: 매도 횟수
- num_hold: 관망 횟수
- immediate_reward: 즉시 보상
- profitloss: 현재 손익
- base_profitloss: 직전 지연 보상 이후 손익
- exploration_base: 탐험 행동 결정 기준 - 1에 가까우면 매수탐험많이, 0에 가까우면 매도
함수)
- reset(): 에이전트의 상태 초기화
- set_balance(): 초기 자본금 설정
- get_states(): 에이전트 상태 획득
- decide_action(): 탐험 또는 정책신경망에 의한 행동 결정
- validate_action(): 행동의 유효성 판단
- decide_trading_unit(): 매수 또는 매도할 주식 수 결정
- act(): 행동 수행
주식보유비율 = 보유 주식수 / (PV 가치 / 현재 주가)
	주식 수 넘 적으면 매수 관점에서 투자, 반대도 성립
	정책신경망의 입력 포함
포트폴리오 가치 비율 = PV / 기준 PV
	0에 가까우면 손실큼, 1에 가까우면 수익 발생
	수익률이 목표수익률에 가까우면 매도의 관점으로 투자
	기준PV: 직전에 목표 수익에 달성했을 떄의 PV
	이 값을 에이전트의 상태로 정함
	정책 신경망의 입력값으로 포함
learners.py - DQNLearner, PolicyGradientLearner, ActorCriticLearner, A2CLearner, 
	A3CLearner
=> 강화학습 기법 기반 클래스들
	RLTrader의 몸체 - 학습 데이터 가지고 보상 결정시 신경망 학습한다
<networks.py - LSTM, CNN, DNN, LSTMNetwork, 완전 연결 심층 신경망>
=> 신경망 클래스들 - 매수/매도 결정하는 에이전트 뇌 역할
	PV증가방향으로. 주식잔고, 주가 등락으로 리스크 관리 및 PV조절
	주식 데이터 in 가치 신경망: 매수/매도시 수익률 예측
	주식 데이터 in 정책 신경망: 매수/매도 확률 예측
속성)
- shared_network: 신경마의 상단부로 여러 신경망이 공유할 수 있다
		ex) A2C에서 정책/가치 신경망이 상단부 공유
- activation: 활성화 함수
- loss: 신경망의 손실 함수
- lr: 신경망의 학습 속도
- model: Keras 라이브러리로 구성한 최종 신경망 모델

함수)
- predict(): 투자 행동별 가치, 확률 계산
- train_on_batch(): 배치 학습 위한 데이터생성
- save_model(): 학습한 신경망 파일로 저장
- load_model(): 저장한 신경망 로드
- get_shared_network(): 신경망의 상단부 생성하는 클래스 함수
visualizer.py - 환경, 에이전트 상태, 가치 신경망 출력, 정책 신경망 출력, PV 등 이미지로 가시화
main.py - 여러 옵션으로 강화학습 실행
=> 종목 코드, 강화학습 기법, 신경망 종류, 학습 속도, 할인률, 탐험률, 초기 자본금, 에포크 수
	등 결정
settings.py - 프로젝트 기본 경로, 로케일..? 등 설정
utils.py - 오늘 날짜, 현재 시간 문자열로 받는 함수 / 시그모이드 함수 등 구현
